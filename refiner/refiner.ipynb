{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import time\n",
    "import os\n",
    "from scipy import linalg\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "FLAG_python = False\n",
    "\n",
    "regular_size = (256, 256)\n",
    "patch_size=(64, 64)\n",
    "patch_number = (regular_size[0]//patch_size[0])*(regular_size[1]//patch_size[1])\n",
    "FeatureNumber = 256\n",
    "\n",
    "def make_regular_image(img):\n",
    "    return img.resize(regular_size).convert('RGB')\n",
    "\n",
    "def calc_hist(img):\n",
    "    w, h = img.size\n",
    "    patch_w, patch_h = patch_size\n",
    "    assert w%patch_w == h%patch_h == 0\n",
    "\n",
    "    hist_dict = dict()\n",
    "    index = 0\n",
    "\n",
    "    for i in xrange(0, w, patch_w):\n",
    "        for j in xrange(0, h, patch_h):\n",
    "            subimg = img.crop((i, j, i+patch_w, j+patch_h)).copy()\n",
    "            hist_dict[index] = subimg.histogram()\n",
    "            index+=1\n",
    "\n",
    "    return hist_dict\n",
    "\n",
    "def _get_files_abspath_from_directory(fdirname):\n",
    "    fname_list = os.listdir(fdirname)\n",
    "    fpath_list = list()\n",
    "\n",
    "    for fname in fname_list:\n",
    "        fpath = fdirname + '/' + fname\n",
    "        if os.path.isfile(fpath):\n",
    "            fpath_list.append(fpath)\n",
    "\n",
    "    return fpath_list\n",
    "\n",
    "def _get_infos_from_textfile(fname):\n",
    "    info_list = list()\n",
    "\n",
    "    fid = open(fname, 'r')\n",
    "    lines = fid.readlines()\n",
    "    for eachline in lines:\n",
    "        eachline = eachline.strip()\n",
    "        if not eachline.startswith('#') and len(eachline):\n",
    "            info_list.append(eachline)\n",
    "    fid.close()\n",
    "\n",
    "    return info_list\n",
    "\n",
    "def _get_images_path(file_path_list):\n",
    "    img_path_list = list()\n",
    "\n",
    "    for filepath in file_path_list:\n",
    "        if not os.path.exists(filepath):\n",
    "            print filepath, 'not exist'\n",
    "            continue\n",
    "\n",
    "        if filepath.endswith('.jpg') or filepath.endswith('.JPEG'):\n",
    "            img_path_list.append(filepath)\n",
    "        else:\n",
    "            print 'Not a jpg or JPEG', filepath\n",
    "\n",
    "    return img_path_list\n",
    "\n",
    "def _extract_image_features(img_path_list):\n",
    "    stime = time.time()\n",
    "    feature_dict = dict()\n",
    "    img_valid_path_list = list()\n",
    "\n",
    "    for patch in range(0, patch_number):\n",
    "        feature_dict[patch] = list()\n",
    "\n",
    "    for index in range(0, len(img_path_list)):\n",
    "        imgpath = img_path_list[index]\n",
    "        try:\n",
    "            img = make_regular_image(Image.open(imgpath))\n",
    "\n",
    "            hist_dict = calc_hist(img)\n",
    "\n",
    "            assert len(hist_dict) == patch_number\n",
    "\n",
    "            img_valid_path_list.append(imgpath)\n",
    "\n",
    "            for patch in range(0, patch_number):\n",
    "                feature_dict[patch].append(hist_dict[patch])\n",
    "\n",
    "        except Exception,e:\n",
    "            print Exception, \":\", e, imgpath\n",
    "            # remove broken images\n",
    "            os.remove(imgpath)\n",
    "\n",
    "        if (index%1000)==0:\n",
    "            print(\"image %d (%d)\" % (index, len(img_path_list)))\n",
    "            print(\"--- extract image features cost %s seconds ---\" % (time.time() - stime))\n",
    "\n",
    "    return feature_dict, img_valid_path_list\n",
    "\n",
    "def PCA(data, dims_rescaled_data):\n",
    "    # mean center the data\n",
    "    m = data.mean(axis=0)\n",
    "    data -= m\n",
    "    # calculate the covariance matrix\n",
    "    R = np.cov(data, rowvar=False)\n",
    "    # calculate eigenvectors and eigenvalues of the covariance matrix\n",
    "    # use 'eigh' rather than 'eig' since R is symmetric\n",
    "    evals, evecs = linalg.eigh(R)\n",
    "    # sort eigenvalue in decreasing order and sort eigenvectors according to same index\n",
    "    indices = np.argsort(evals)[::-1]\n",
    "    evals = evals[indices]\n",
    "    evecs = evecs[:, indices]\n",
    "    # select the first n eigenvectors (n is desired dimension of rescaled data array, or dims_rescaled_data)\n",
    "    if dims_rescaled_data:\n",
    "        evecs = evecs[:,:dims_rescaled_data]\n",
    "        print np.sum(evals[:dims_rescaled_data])/np.sum(evals)\n",
    "    # carry out the transformation on the data using eigenvectors\n",
    "    # and return the re-scaled data, eigenvectors, and data mean\n",
    "    return np.dot(data, evecs), evecs, m\n",
    "\n",
    "def _feature_compression(feature_dict, dims):\n",
    "    compressed_feature_dict = dict()\n",
    "    evecs_dict = dict()\n",
    "    imgmean_dict = dict()\n",
    "\n",
    "    for patch in range(0, patch_number):\n",
    "        compressed_feature_dict[patch], evecs_dict[patch], imgmean_dict[patch] = PCA(np.array(feature_dict[patch]), dims)\n",
    "\n",
    "    return compressed_feature_dict, evecs_dict, imgmean_dict\n",
    "\n",
    "def _feature_projection(feature_dict, evecs_dict, imgmean_dict):\n",
    "    projected_feature_dict = dict()\n",
    "\n",
    "    for patch in range(0, patch_number):\n",
    "        projected_feature_dict[patch] = np.dot((feature_dict[patch] - imgmean_dict[patch]), evecs_dict[patch])\n",
    "\n",
    "    return projected_feature_dict\n",
    "\n",
    "def _calc_corr_coef_basic(feature_dict):\n",
    "    corr_coef = np.corrcoef(feature_dict[0])\n",
    "    for patch in range(1, patch_number):\n",
    "        corr_coef += np.corrcoef(feature_dict[patch])\n",
    "    # remove diagonal elements and corrcoef is symmetric matrix, get lower triangle of a symmetric matrix\n",
    "    corr_coef = np.tril(corr_coef, -1)\n",
    "\n",
    "    return corr_coef/patch_number\n",
    "\n",
    "def _calc_corr_coef_advance(feature_dict_1, feature_dict_2):\n",
    "    corr_coef = np.corrcoef(feature_dict_1[0], feature_dict_2[0])\n",
    "    for patch in range(1, patch_number):\n",
    "        corr_coef += np.corrcoef(feature_dict_1[patch], feature_dict_2[patch])\n",
    "    # get top-right area\n",
    "    corr_coef = corr_coef[0:feature_dict_1[0].shape[0], feature_dict_1[0].shape[0]:corr_coef.shape[0]]\n",
    "\n",
    "    return corr_coef/patch_number\n",
    "\n",
    "def _get_similar_items_basic(corr_coef, thres):\n",
    "    simi_item_pair_list = np.argwhere(corr_coef > thres)\n",
    "    print('threshold %.2f simi_item_pair_list size %d' % (thres, len(simi_item_pair_list)))\n",
    "\n",
    "    simi_set_dict = dict()\n",
    "    simi_item_dict = dict()\n",
    "\n",
    "    for item in simi_item_pair_list:\n",
    "        flag_found = False\n",
    "        for index in range(0, len(simi_set_dict)):\n",
    "            if set(item) & simi_set_dict[index]:\n",
    "                simi_set_dict[index].update(set(item))\n",
    "                flag_found = True\n",
    "\n",
    "        if not flag_found:\n",
    "            simi_set_dict[len(simi_set_dict)] = set(item)\n",
    "\n",
    "    for index in simi_set_dict:\n",
    "        item_list = list(simi_set_dict[index])\n",
    "        simi_item_dict[item_list[0]] = item_list[1:len(item_list)]\n",
    "\n",
    "    print('threshold %.2f simi_item_dict size %d' % (thres, len(simi_item_dict)))\n",
    "    return simi_item_dict\n",
    "\n",
    "def _get_similar_items_advance(corr_coef, thres, offset1, offset2):\n",
    "    simi_item_pair_list = np.argwhere(corr_coef > thres)\n",
    "    print('threshold %.2f simi_item_pair_list size %d' % (thres, len(simi_item_pair_list)))\n",
    "\n",
    "    simi_item_dict = dict()\n",
    "\n",
    "    for item in simi_item_pair_list:\n",
    "        gt_id = item[0] + offset1\n",
    "        candidate_id = item[1] + offset2\n",
    "\n",
    "        if not simi_item_dict.has_key(gt_id):\n",
    "            simi_item_dict[gt_id] = list()\n",
    "\n",
    "        simi_item_dict[gt_id].append(candidate_id)\n",
    "\n",
    "    print('threshold %.2f simi_item_dict size %d' % (thres, len(simi_item_dict)))\n",
    "    return simi_item_dict\n",
    "\n",
    "def combine_2_similar_item_dict(dict1, dict2):\n",
    "    tempdict = dict1.copy()\n",
    "\n",
    "    for key in dict2:\n",
    "        if key in dict1:\n",
    "            tempdict[key].extend(dict2[key])\n",
    "        else:\n",
    "            tempdict[key] = dict2[key]\n",
    "\n",
    "    return tempdict\n",
    "\n",
    "def _get_similar_basic(corr_coef, threshold):\n",
    "    tempdict = dict()\n",
    "\n",
    "    for thres in np.arange(threshold, 0.91, 0.05):\n",
    "        tempdict[thres] = _get_similar_items_basic(corr_coef, thres)\n",
    "\n",
    "    return tempdict\n",
    "\n",
    "def _get_similar_advance(corr_coef, threshold, offset1, offset2, dict1):\n",
    "    tempdict = dict1.copy()\n",
    "\n",
    "    for thres in np.arange(threshold, 0.91, 0.05):\n",
    "        simi_item_dict = _get_similar_items_advance(corr_coef, thres, offset1, offset2)\n",
    "\n",
    "        if thres not in tempdict:\n",
    "            tempdict[thres] = dict()\n",
    "\n",
    "        tempdict[thres] = combine_2_similar_item_dict(tempdict[thres], simi_item_dict)\n",
    "\n",
    "    return tempdict\n",
    "\n",
    "def _save_refined_infos_basic(img_path_list, simi_dict, fprefix):\n",
    "    # img_path_list is candidate set\n",
    "    assert len(img_path_list), 'the length of img_path_list is smaller than 0'\n",
    "    assert len(simi_dict), 'the length of simi_dict is smaller than 0'\n",
    "\n",
    "    for thres in simi_dict:\n",
    "        fname_discard = fprefix + '_discard_' + str(thres)\n",
    "        fname_keep = fprefix + '_keep_' + str(thres)\n",
    "\n",
    "        fid_discard = open(fname_discard, 'w')\n",
    "        fid_discard.write('### images discard threshold: ' + str(thres) + ' ###' + '\\n')\n",
    "\n",
    "        img_discard_set = set()\n",
    "\n",
    "        simi_item_dict = simi_dict[thres]\n",
    "\n",
    "        index = 0\n",
    "        for img_1_id in simi_item_dict:\n",
    "            img_2_id_list = list(set(simi_item_dict[img_1_id]))\n",
    "            if len(img_2_id_list):\n",
    "                fid_discard.write('###'+ str(index) +'###' + '\\n')\n",
    "                fid_discard.write('# ' + img_path_list[img_1_id] + '\\n')\n",
    "                index += 1\n",
    "                for img_2_id in img_2_id_list:\n",
    "                    fid_discard.write(img_path_list[img_2_id] + '\\n')\n",
    "\n",
    "                img_discard_set.update(set(img_2_id_list))\n",
    "        fid_discard.close()\n",
    "\n",
    "        fid_keep = open(fname_keep, 'w')\n",
    "        fid_keep.write('### images keep +' + str(len(img_path_list) - len(img_discard_set)) + '(-' + str(len(img_discard_set)) + ') threshold: ' + str(thres) + ' ###' + '\\n')\n",
    "        for img_2_id in range(0, len(img_path_list)):\n",
    "            if img_2_id not in img_discard_set:\n",
    "                fid_keep.write(img_path_list[img_2_id] + '\\n')\n",
    "        fid_keep.close()\n",
    "\n",
    "        print(\"threshold %.2f candidate images discarded size %d kept size %d\" % (thres, len(img_discard_set), len(img_path_list)-len(img_discard_set)))\n",
    "\n",
    "def _save_refined_infos_advance(img_path_list_1, img_path_list_2, simi_dict, fprefix):\n",
    "    # img_path_list_1 is authority gt set, img_path_list_2 is candidate set\n",
    "    assert len(img_path_list_1), 'the length of img_path_list_1 is smaller than 0'\n",
    "    assert len(img_path_list_2), 'the length of img_path_list_2 is smaller than 0'\n",
    "    assert len(simi_dict), 'the length of simi_dict is smaller than 0'\n",
    "\n",
    "    for thres in simi_dict:\n",
    "        fname_discard = fprefix + '_discard_' + str(thres)\n",
    "        fname_keep = fprefix + '_keep_' + str(thres)\n",
    "\n",
    "        fid_discard = open(fname_discard, 'w')\n",
    "        fid_discard.write('### images discard threshold: ' + str(thres) + ' ###' + '\\n')\n",
    "\n",
    "        img_discard_set = set()\n",
    "\n",
    "        simi_item_dict = simi_dict[thres]\n",
    "        index = 0\n",
    "        for img_1_id in simi_item_dict:\n",
    "            img_2_id_list = list(set(simi_item_dict[img_1_id]))\n",
    "            if len(img_2_id_list):\n",
    "                fid_discard.write('###'+ str(index) +'###' + '\\n')\n",
    "                fid_discard.write('# ' + img_path_list_1[img_1_id] + '\\n')\n",
    "                index += 1\n",
    "                for img_2_id in img_2_id_list:\n",
    "                    fid_discard.write(img_path_list_2[img_2_id] + '\\n')\n",
    "\n",
    "                img_discard_set.update(set(img_2_id_list))\n",
    "        fid_discard.close()\n",
    "\n",
    "        fid_keep = open(fname_keep, 'w')\n",
    "        fid_keep.write('### images keep +' + str(len(img_path_list_2) - len(img_discard_set)) + '(-' + str(len(img_discard_set)) + ') threshold: ' + str(thres) + ' ###' + '\\n')\n",
    "        for img_2_id in range(0, len(img_path_list_2)):\n",
    "            if img_2_id not in img_discard_set:\n",
    "                fid_keep.write(img_path_list_2[img_2_id] + '\\n')\n",
    "        fid_keep.close()\n",
    "\n",
    "        print(\"threshold %.2f candidate images discarded size %d kept size %d\" % (thres, len(img_discard_set), len(img_path_list_2)-len(img_discard_set)))\n",
    "\n",
    "def refiner_advance(gt_file_list, candidate_file_list, out_fprefix, threshold):\n",
    "    stime = time.time()\n",
    "    print '*************** get images path *****************'\n",
    "    gt_images_list = _get_images_path(gt_file_list)\n",
    "    candidate_images_list = _get_images_path(candidate_file_list)\n",
    "    print(\"--- get images path cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "    # limit the candidate_images_list size up to 10000\n",
    "    if len(candidate_images_list) > 10000:\n",
    "        candidate_images_list = candidate_images_list[0:10000]\n",
    "        #candidate_images_list = random.sample(candidate_images_list, 10000)\n",
    "\n",
    "\n",
    "    ### split candidate images into multiple batch\n",
    "    candidate_batch_size = 10000\n",
    "    candidate_images_number = len(candidate_images_list)\n",
    "    candidate_batch_number = candidate_images_number // candidate_batch_size\n",
    "    candidate_batch_number = (candidate_batch_number+1) if (candidate_images_number % candidate_batch_size) else candidate_batch_number\n",
    "\n",
    "    #store valid images list\n",
    "    candidate_images_valid_list = list()\n",
    "    gt_images_valid_list = list()\n",
    "    # store similar for each threshold\n",
    "    similar_dict = dict()\n",
    "\n",
    "    print(\"--- split candidate images (%d) into %d batch (%d)---\" % (candidate_images_number, candidate_batch_number, candidate_batch_size))\n",
    "\n",
    "    for candidate_batch_id in range(0, candidate_batch_number):\n",
    "        candidate_id_start = candidate_batch_id * candidate_batch_size\n",
    "        candidate_id_end = (candidate_batch_id+1) * candidate_batch_size\n",
    "        if candidate_id_end > candidate_images_number:\n",
    "            candidate_id_end = candidate_images_number\n",
    "\n",
    "        print (\"--- candidate batch %d images from %d to %d ---\" % (candidate_batch_id, candidate_id_start, candidate_id_end))\n",
    "        candidate_batch_images_list = candidate_images_list[candidate_id_start:candidate_id_end]\n",
    "\n",
    "        stime = time.time()\n",
    "        print '*************** extract candidate images features *****************'\n",
    "        candidate_batch_images_feature_dict, candidate_batch_images_valid_list = _extract_image_features(candidate_batch_images_list)\n",
    "        print(\"--- extract candidate images feature cost %s seconds ---\" % (time.time() - stime))\n",
    "        print ''\n",
    "\n",
    "        ### split gt images into multiple batch\n",
    "        gt_batch_size = 6000\n",
    "        gt_images_number = len(gt_images_list)\n",
    "        gt_batch_number = gt_images_number // gt_batch_size\n",
    "        gt_batch_number = (gt_batch_number+1) if (gt_images_number % gt_batch_size) else gt_batch_number\n",
    "\n",
    "        print(\"--- split gt images (%d) into %d batch (%d)---\" % (gt_images_number, gt_batch_number, gt_batch_size))\n",
    "\n",
    "        for gt_batch_id in range(0, gt_batch_number):\n",
    "            gt_id_start = gt_batch_id * gt_batch_size\n",
    "            gt_id_end = (gt_batch_id+1) * gt_batch_size\n",
    "            if gt_id_end > gt_images_number:\n",
    "                gt_id_end = gt_images_number\n",
    "\n",
    "            print (\"--- gt batch %d images from %d to %d ---\" % (gt_batch_id, gt_id_start, gt_id_end))\n",
    "            gt_batch_images_list = gt_images_list[gt_id_start:gt_id_end]\n",
    "\n",
    "            stime = time.time()\n",
    "            print '*************** extract gt images features *****************'\n",
    "            gt_batch_images_feature_dict, gt_batch_images_valid_list = _extract_image_features(gt_batch_images_list)\n",
    "            print(\"--- extract gt images feature cost %s seconds ---\" % (time.time() - stime))\n",
    "            print ''\n",
    "\n",
    "            stime = time.time()\n",
    "            print '*************** compress gt images features *****************'\n",
    "            gt_batch_images_compressed_feature_dict, gt_batch_eigenvecs_dict, gt_batch_images_mean_dict = _feature_compression(gt_batch_images_feature_dict, FeatureNumber)\n",
    "            print(\"--- compress gt images feature cost %s seconds ---\" % (time.time() - stime))\n",
    "            print ''\n",
    "\n",
    "            stime = time.time()\n",
    "            print '*************** project candidate images features *****************'\n",
    "            candidate_batch_images_projected_feature_dict = _feature_projection(candidate_batch_images_feature_dict, gt_batch_eigenvecs_dict, gt_batch_images_mean_dict)\n",
    "            print(\"--- project candidate images feature cost %s seconds ---\" % (time.time() - stime))\n",
    "            print ''\n",
    "\n",
    "            stime = time.time()\n",
    "            print '*************** calculate corrcoef *****************'\n",
    "            # gt_batch_images_compressed_feature_dict must be arg1, candidate_batch_images_projected_feature_dict must be arg2\n",
    "            corr_coefs = _calc_corr_coef_advance(gt_batch_images_compressed_feature_dict, candidate_batch_images_projected_feature_dict)\n",
    "            print(\"--- calculate corrcoef cost %s seconds ---\" % (time.time() - stime))\n",
    "            print ''\n",
    "            print 'corr_coefs', corr_coefs.shape\n",
    "\n",
    "            stime = time.time()\n",
    "            print '*************** get similar dict *****************'\n",
    "            similar_dict = _get_similar_advance(corr_coefs, threshold, len(gt_images_valid_list), len(candidate_images_valid_list), similar_dict)\n",
    "            print(\"--- get similar dict cost %s seconds ---\" % (time.time() - stime))\n",
    "            print ''\n",
    "\n",
    "            gt_images_valid_list.extend(gt_batch_images_valid_list)\n",
    "\n",
    "        candidate_images_valid_list.extend(candidate_batch_images_valid_list)\n",
    "\n",
    "    stime = time.time()\n",
    "    print '*************** save refined images infos *****************'\n",
    "    # gt_images_valid_list must be arg1, candidate_images_valid_list must be arg2\n",
    "    _save_refined_infos_advance(gt_images_valid_list, candidate_images_valid_list, similar_dict, out_fprefix)\n",
    "    print(\"--- save refined images infos cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "def refiner_basic(candidate_file_list, out_fprefix, threshold):\n",
    "    stime = time.time()\n",
    "    print '*************** get images path *****************'\n",
    "    candidate_images_list = _get_images_path(candidate_file_list)\n",
    "    print(\"--- get images path cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "    # limit the candidate_images_list size up to 15000\n",
    "    if len(candidate_images_list) > 18000:\n",
    "        candidate_images_list = candidate_images_list[0:18000]\n",
    "        #candidate_images_list = random.sample(candidate_images_list, 14000)\n",
    "\n",
    "    stime = time.time()\n",
    "    print '*************** extract candidate images features *****************'\n",
    "    candidate_images_feature_dict, candidate_images_valid_list = _extract_image_features(candidate_images_list)\n",
    "    print(\"--- extract candidate images feature cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "    stime = time.time()\n",
    "    print '*************** compress candidate images features *****************'\n",
    "    candidate_images_compressed_feature_dict, candidate_eigenvecs_dict, candidate_images_mean_dict = _feature_compression(candidate_images_feature_dict, FeatureNumber)\n",
    "    print(\"--- compress candidate images feature cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "    stime = time.time()\n",
    "    print '*************** calculate corrcoef *****************'\n",
    "    corr_coefs = _calc_corr_coef_basic(candidate_images_compressed_feature_dict)\n",
    "    print(\"--- calculate corrcoef cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "    print 'corr_coefs', corr_coefs.shape\n",
    "\n",
    "    stime = time.time()\n",
    "    print '*************** get similar dict *****************'\n",
    "    similar_dict = _get_similar_basic(corr_coefs, threshold)\n",
    "    print(\"--- get similar dict cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "    stime = time.time()\n",
    "    print '*************** save refined images infos *****************'\n",
    "    _save_refined_infos_basic(candidate_images_valid_list, similar_dict, out_fprefix)\n",
    "    print(\"--- save refined images infos cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "def _get_train_val_file_list(infos):\n",
    "    synsets_path = '/home/lpzhang/Desktop/ImageNetData/synsets_200.txt'\n",
    "    train_dprefix = '/home/lpzhang/Desktop/ImageNetData/ImageNet/ILSVRC2014/ILSVRC2013_DET_train'\n",
    "    val_fprefix = '/home/lpzhang/Desktop/ImageNetData/ilsvrc1314_val/ilsvrc1314_val_cls_'\n",
    "    synsets = dict()\n",
    "    train_val_file_list = list()\n",
    "    # synsets\n",
    "    synsets_infos = _get_infos_from_textfile(synsets_path)\n",
    "    for eachinfo in synsets_infos:\n",
    "        eachinfo = eachinfo.split()\n",
    "        synsets[eachinfo[1]] = eachinfo[0]\n",
    "\n",
    "    for i in range(0, len(infos)):\n",
    "        # train data\n",
    "        wnid = infos[i]\n",
    "        train_dir = os.path.normpath(train_dprefix) + '/' + wnid\n",
    "        train_val_file_list.extend(_get_files_abspath_from_directory(train_dir))\n",
    "        print(\"wnid %s image size %d\" % (wnid, len(train_val_file_list)))\n",
    "\n",
    "        # val data\n",
    "        if synsets.has_key(wnid):\n",
    "            clsid = synsets[wnid]\n",
    "            fval_path = val_fprefix + clsid + '.txt'\n",
    "            train_val_file_list.extend(_get_infos_from_textfile(fval_path))\n",
    "            print(\"clsid %s wnid %s image size %d\" % (clsid, wnid, len(train_val_file_list)))\n",
    "\n",
    "    return train_val_file_list\n",
    "\n",
    "def _get_candidate_file_list(infos):\n",
    "    file_path_list = list()\n",
    "    file_id_list = list()\n",
    "\n",
    "    for index in range(0, len(infos)):\n",
    "        print(\"File %d (%d)\" % (index+1, len(infos)))\n",
    "        fpath = infos[index]\n",
    "        fdir, fext = os.path.splitext(fpath)\n",
    "        id_url_list = _get_infos_from_textfile(fpath)\n",
    "        for id_url in id_url_list:\n",
    "            id_url = id_url.split()\n",
    "            if id_url[0] not in file_id_list:\n",
    "                file_id_list.append(id_url[0])\n",
    "                file_path_list.append(os.path.normpath(fdir) + '/' + id_url[0] + '.jpg')\n",
    "\n",
    "    return file_path_list\n",
    "\n",
    "def refine_from_itself(infos, threshold, dtype):\n",
    "    ### each infos contain same object candidate files\n",
    "    candidate_fpath = os.path.normpath(infos[0])\n",
    "    out_fprefix = candidate_fpath + '_' + dtype\n",
    "\n",
    "    for candidate_fpath in infos:\n",
    "        assert os.path.exists(candidate_fpath), 'candidate_fpath not exist'\n",
    "        assert os.path.isfile(candidate_fpath), 'candidate_fpath is not a file'\n",
    "        print 'candidate_fpath:', candidate_fpath\n",
    "\n",
    "    print 'out_fprefix:', out_fprefix\n",
    "\n",
    "    # get file list\n",
    "    stime = time.time()\n",
    "    print '*************** get candidate file list *****************'\n",
    "    candidate_files = _get_candidate_file_list(infos)\n",
    "    print(\"--- get candidate file list cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "    # call the refiner_basic\n",
    "    refiner_basic(candidate_files, out_fprefix, threshold)\n",
    "\n",
    "def refine_from_det_train_val(infos, threshold, dtype):\n",
    "    candidate_fpath = os.path.normpath(infos[0])\n",
    "    out_fprefix = candidate_fpath + '_' + dtype\n",
    "\n",
    "    assert os.path.exists(candidate_fpath), 'candidate_fpath not exist'\n",
    "    assert os.path.isfile(candidate_fpath), 'candidate_fpath is not a file'\n",
    "\n",
    "    print 'candidate_fpath:', candidate_fpath\n",
    "    print 'out_fprefix:', out_fprefix\n",
    "\n",
    "    # get file list\n",
    "    stime = time.time()\n",
    "    print '*************** get file list *****************'\n",
    "    candidate_files = _get_infos_from_textfile(candidate_fpath)\n",
    "    infos = infos[1:len(infos)]\n",
    "    gt_files = _get_train_val_file_list(infos)\n",
    "    print(\"--- get get file list cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "    # call the refiner_advance\n",
    "    refiner_advance(gt_files, candidate_files, out_fprefix, threshold)\n",
    "\n",
    "def refine_from_det_test(infos, threshold, dtype):\n",
    "    # each infos contains candidate file path, gt file path or gtdir\n",
    "    candidate_fpath = os.path.normpath(infos[0])\n",
    "    assert os.path.exists(candidate_fpath), 'candidate_fpath not exist'\n",
    "    assert os.path.isfile(candidate_fpath), 'candidate_fpath is not a file'\n",
    "    out_fprefix = candidate_fpath + '_' + dtype\n",
    "    gt_fdir = os.path.normpath(infos[1])\n",
    "    assert os.path.exists(gt_fdir), 'gt_fdir not exist'\n",
    "    assert os.path.isdir(gt_fdir), 'gt_fdir is not a dir'\n",
    "\n",
    "    print 'candidate_fpath:', candidate_fpath\n",
    "    print 'out_fprefix:', out_fprefix\n",
    "\n",
    "    # get file list\n",
    "    stime = time.time()\n",
    "    print '*************** get file list *****************'\n",
    "    candidate_files = _get_infos_from_textfile(candidate_fpath)\n",
    "    gt_files = _get_files_abspath_from_directory(gt_fdir)\n",
    "    print(\"--- get get file list cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "\n",
    "    # call the refiner_advance\n",
    "    refiner_advance(gt_files, candidate_files, out_fprefix, threshold)\n",
    "    \n",
    "def refine_from_others(infos, threshold, dtype):\n",
    "    # infos[0] contains candidate file path\n",
    "    # infos[1:] contains gt file path or dir\n",
    "    # check fpath\n",
    "    candidate_fpath = os.path.normpath(infos[0])\n",
    "    print 'candidate_fpath:', candidate_fpath\n",
    "    assert os.path.exists(candidate_fpath), 'candidate_fpath not exist'\n",
    "    assert os.path.isfile(candidate_fpath), 'candidate_fpath is not a file'\n",
    "    out_fprefix = candidate_fpath + '_' + dtype\n",
    "    for i in range(1, len(infos)):\n",
    "        gt_fpath = os.path.normpath(infos[i])\n",
    "        print('gt_fpath %d: %s' % (i, gt_fpath))\n",
    "        assert os.path.exists(gt_fpath), 'gt_fpath not exist'\n",
    "        assert (os.path.isfile(gt_fpath) or os.path.isdir(gt_fpath)), 'gt_fpath must be either file or dir'\n",
    "        \n",
    "    # get candidate file list\n",
    "    stime = time.time()\n",
    "    print '*************** get candidate file list *****************'\n",
    "    candidate_files = _get_infos_from_textfile(candidate_fpath)\n",
    "    # get gt file list\n",
    "    print '*************** get gt file list *****************'\n",
    "    gt_files = list()\n",
    "    for i in range(1, len(infos)):\n",
    "        gt_fpath = os.path.normpath(infos[i])\n",
    "        if os.path.isfile(gt_fpath):\n",
    "            gt_files.extend(_get_infos_from_textfile(gt_fpath))\n",
    "        else:\n",
    "            gt_files.extend(_get_files_abspath_from_directory(gt_fpath))\n",
    "            \n",
    "    print(\"--- get file list cost %s seconds ---\" % (time.time() - stime))\n",
    "    print ''\n",
    "    \n",
    "    # call the refiner_advance\n",
    "    refiner_advance(gt_files, candidate_files, out_fprefix, threshold)\n",
    "    \n",
    "def refine_wrapper(infos, threshold, dtype):\n",
    "    if dtype == 'ITSELF':\n",
    "        print 'refine from itself'\n",
    "        refine_from_itself(infos, threshold, dtype)\n",
    "    elif dtype == 'TRAIN':\n",
    "        print 'refine from train'\n",
    "        refine_from_det_train_val(infos, threshold, dtype)\n",
    "    else:\n",
    "        print 'refine from test'\n",
    "        refine_from_det_test(infos, threshold, dtype)\n",
    "\n",
    "def main(arg):\n",
    "    start_time = time.time()\n",
    "    if FLAG_python is True:\n",
    "        # for python run in command-line\n",
    "        dtype = str(args.dtype)\n",
    "        fpath = os.path.normpath(args.fpath)\n",
    "        threshold = float(args.threshold)\n",
    "        print 'Python'\n",
    "    else:\n",
    "        # for jupyter\n",
    "        dtype = str(args['dtype'])\n",
    "        fpath = os.path.normpath(args['fpath'])\n",
    "        threshold = float(args['threshold'])\n",
    "        print 'Jupyter'\n",
    "    \n",
    "    # check the args\n",
    "    if not os.path.isabs(fpath):\n",
    "        fpath = os.path.abspath(fpath)\n",
    "    print 'dtype:', dtype\n",
    "    print 'fpath:', fpath\n",
    "    print 'threshold:', threshold\n",
    "    assert dtype in ['ITSELF', 'TRAIN', 'TEST'], 'dtype undefined'\n",
    "    assert type(threshold) is float, 'image_number must be float'\n",
    "    assert (float >= 0.0 and image_number <= 1.0), 'image_number must between 0 and 1'\n",
    "    assert os.path.exists(fpath), 'fpath not exist'\n",
    "    assert os.path.isfile(fpath), 'fpath is not a file'\n",
    "    print 'args checked\\n'\n",
    "    \n",
    "    print '++++++++++++++++++++++++++ START +++++++++++++++++++++++++++++++++'\n",
    "    infos = _get_infos_from_textfile(fpath)\n",
    "    for eachinfo in infos:\n",
    "        eachinfo = eachinfo.split()\n",
    "        refine_wrapper(eachinfo, threshold, dtype)\n",
    "\n",
    "    print(\"------------- total cost %s seconds ----------\" % (time.time() - start_time))\n",
    "    print '++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++'\n",
    "\n",
    "if FLAG_python is True:\n",
    "    if __name__ == \"__main__\":\n",
    "        parser = ArgumentParser(description=\"Refine Images\")\n",
    "        parser.add_argument('-dtype', required=True)\n",
    "        parser.add_argument('-fpath', required=True)\n",
    "        parser.add_argument('-threshold', required=True)\n",
    "        args = parser.parse_args()\n",
    "        main(args)\n",
    "else:\n",
    "    args = {}\n",
    "    args['dtype'] = 'ITSELF' # TEST, TRAIN\n",
    "    args['fpath'] = 'refine_itself.txt'\n",
    "    args['threshold'] = 0.5\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
