{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "import errno\n",
    "import random\n",
    "import string\n",
    "from argparse import ArgumentParser\n",
    "import time\n",
    "\n",
    "FLAG_python = False\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def _mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc: # Python >2.5 (except OSError, exc: for Python <2.5)\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def _get_random_id():\n",
    "    characters = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a','b','c','d','e','f','g','h','i','j','k','l','m','n']\n",
    "    id_element = random.sample(characters, 8)\n",
    "    random_id = ''.join(id_element)\n",
    "    return random_id\n",
    "\n",
    "def _get_infos_from_textfile(fname):\n",
    "    info_list = list()\n",
    "\n",
    "    fid = open(fname, 'r')\n",
    "    lines = fid.readlines()\n",
    "    for eachline in lines:\n",
    "        eachline = eachline.strip()\n",
    "        if not eachline.startswith('#') and len(eachline):\n",
    "            info_list.append(eachline)\n",
    "    fid.close()\n",
    "\n",
    "    return info_list\n",
    "\n",
    "def entryurl_gettyimages(page_index, keyword):\n",
    "    # entry_url = 'http://www.gettyimages.com/photos/bow-arrows?excludenudity=true&page=2&phrase=bow%20arrows&sort=best'\n",
    "    keyword_form_1 = '-'.join(keyword.split())\n",
    "    keyword_form_2 = '%20'.join(keyword.split())\n",
    "\n",
    "    entryurl = 'http://www.gettyimages.com/photos/' + keyword_form_1 + '?excludenudity=true&page=' + str(page_index) + '&phrase=' + keyword_form_2 + '&sort=best'\n",
    "\n",
    "    return entryurl\n",
    "\n",
    "def entryurl_flickr(page_index, keyword):\n",
    "    keyword_form = '%20'.join(keyword.split())\n",
    "#     keyword_form = '-'.join(keyword.split())\n",
    "    valid_api_key = 'ac22c904b967ae887c969c9ba5a4d7f5'\n",
    "    url_head = 'https://api.flickr.com/services/rest?sort=relevance&parse_tags=1&content_type=7&extras=can_comment%2Ccount_comments%2Ccount_faves%2Cdescription%2Cisfavorite%2Clicense%2Cmedia%2Cneeds_interstitial%2Cowner_name%2Cpath_alias%2Crealname%2Crotation%2Curl_c%2Curl_l%2Curl_m%2Curl_n%2Curl_q%2Curl_s%2Curl_sq%2Curl_t%2Curl_z%2Cis_marketplace_licensable&'\n",
    "    url_per_page = 'per_page=1000'\n",
    "    url_page_index = '&page=' + str(page_index)\n",
    "    #url_search_condition = 'dimension_search_mode=min&height=640&width=640&advanced=1&media=photos&text=rubber%20eraser'\n",
    "    url_search_condition = '&lang=en-US&dimension_search_mode=min&height=640&width=640&media=photos&text=' + keyword_form\n",
    "    url_api_key = '&viewerNSID=&method=flickr.photos.search&csrf=&api_key=' + valid_api_key\n",
    "    url_reqId = '&format=json&hermes=1&hermesClient=1&reqId=' + _get_random_id()\n",
    "    url_end = '&nojsoncallback=1'\n",
    "\n",
    "    entryurl = url_head + url_per_page + url_page_index + url_search_condition + url_api_key + url_reqId + url_end\n",
    "\n",
    "    return entryurl\n",
    "\n",
    "def entryurl_istockphoto(page_index, keyword):\n",
    "    keyword_form_1 = '+'.join(keyword.split())\n",
    "    keyword_form_2 = '%20'.join(keyword.split())\n",
    "\n",
    "    url_head = 'http://www.istockphoto.com/hk/photos/'\n",
    "    url_search_condition = keyword_form_1 + '?facets=%7B%22text%22:%5B%22' + keyword_form_2\n",
    "    url_page_index = '%22%5D,%22pageNumber%22:' + str(page_index)\n",
    "    url_end = ',%22perPage%22:1000,%22abstractType%22:%5B%22photos%22%5D,%22order%22:%22bestMatch%22,%22f%22:true%7D'\n",
    "\n",
    "    entryurl = url_head + url_search_condition + url_page_index + url_end\n",
    "\n",
    "    return entryurl\n",
    "\n",
    "def entryurl_dreamstime(page_index, keyword):\n",
    "    search_keyword = '%20'.join(keyword.split())\n",
    "    url_head = 'https://www.dreamstime.com/search.php?srh_field='\n",
    "    url_end = '&s_ph=y&s_st=new&s_sm=all&s_rsf=0&s_rst=7&s_mrg=1&s_sl0=y&s_sl1=y&s_sl2=y&s_sl3=y&s_sl4=y&s_sl5=y&s_clc=y&s_clm=y&s_orp=y&s_ors=y&s_orl=y&s_orw=y&s_mrc1=y&s_mrc2=y&s_mrc3=y&s_mrc4=y&s_mrc5=y&s_exc=&items=1000&pg='\n",
    "#     url_head = 'http://www.dreamstime.com/search.php?srh_field='\n",
    "#     url_end = '&s_ph=y&s_il=y&s_rf=y&s_ed=y&s_clc=y&s_clm=y&s_orp=y&s_ors=y&s_orl=y&s_orw=y&s_st=new&s_sm=all&s_rsf=0&s_rst=7&s_mrg=1&s_sl0=y&s_sl1=y&s_sl2=y&s_sl3=y&s_sl4=y&s_sl5=y&s_mrc1=y&s_mrc2=y&s_mrc3=y&s_mrc4=y&s_mrc5=y&s_exc=&items=1000&pg='\n",
    "\n",
    "    entryurl = url_head + search_keyword + url_end + str(page_index)\n",
    "\n",
    "    return entryurl\n",
    "\n",
    "def entryurl_pond5(page_index, keyword):\n",
    "    search_keyword = '-'.join(keyword.split())\n",
    "    entryurl = 'https://www.pond5.com/photos/' + str(page_index) + '/' + search_keyword + '.html'\n",
    "\n",
    "    return entryurl\n",
    "\n",
    "def response_contents(url):\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36'}\n",
    "    request = urllib2.Request(url, headers = headers)\n",
    "    try:\n",
    "        response = urllib2.urlopen(request)\n",
    "        content = response.read()     \n",
    "    except urllib2.HTTPError as err:\n",
    "        content = ''\n",
    "        if err.code == 404:\n",
    "            print 'HTTP Error 404: Not Found'\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "    return content\n",
    "\n",
    "def extract_infos_gettyimages(content):\n",
    "    urlprefix = 'http://media.gettyimages.com/photos/id'\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.full.pagination')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.details-wrap .asset-link')\n",
    "\n",
    "    pages_pattern = re.compile(r'page-count=\"(\\d*)\"')\n",
    "    id_pattern = re.compile(r'data-asset-id=\"(.*?)\"')\n",
    "    pages_find = re.findall(pages_pattern, str(content1))\n",
    "    id_find = re.findall(id_pattern, str(content2))\n",
    "\n",
    "    total_pages = int(pages_find[0]) if len(pages_find) else 0\n",
    "\n",
    "    for imgid in id_find:\n",
    "        if imgid not in img_id_url_dict:\n",
    "            imgurl = urlprefix + imgid\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_flickr(content):\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content = BeautifulSoup(content,\"lxml\").find('p').getText()\n",
    "\n",
    "    pages_pattern = re.compile(r'\"pages\":(\\d*),')\n",
    "    url_pattern = re.compile(r',\"url_l\":\"(.*?)\\.jpg\",')\n",
    "    id_pattern = re.compile(r'\\/(\\d*)_')\n",
    "    pages_find = re.findall(pages_pattern, str(content))\n",
    "    url_find = re.findall(url_pattern, str(content))\n",
    "    \n",
    "\n",
    "    total_pages = int(pages_find[0]) if len(pages_find) else 0\n",
    "\n",
    "    for eachurl in url_find:\n",
    "        id_find = re.findall(id_pattern, eachurl)\n",
    "        if len(id_find):\n",
    "            imgid = id_find[0]\n",
    "            if imgid not in img_id_url_dict:\n",
    "                imgurl = eachurl.replace(\"\\/\", \"/\") + '.jpg'\n",
    "                img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_istockphoto(content):\n",
    "    urlprefix = 'http://media.istockphoto.com/photos/id'\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.file-count-label')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.figure-holder')\n",
    "\n",
    "    imgnum_pattern = re.compile(r'>(\\d*)</span>')\n",
    "    id_pattern = re.compile(r'-gm(\\d*)-')\n",
    "    imgnum_find = re.findall(imgnum_pattern, str(content1))\n",
    "    id_find = re.findall(id_pattern, str(content2))\n",
    "\n",
    "    imgnum = imgnum_find[0].split(',')\n",
    "    imgnum = ''.join(imgnum)\n",
    "    imgnum = int(imgnum) if len(imgnum) else 0\n",
    "    \n",
    "    per_page = 1000\n",
    "    total_pages = (imgnum//per_page + 1) if (imgnum%per_page) else (imgnum//per_page)\n",
    "    \n",
    "    for imgid in id_find:\n",
    "        if imgid not in img_id_url_dict:\n",
    "            imgurl = urlprefix + imgid\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_dreamstime(content):\n",
    "    urlprefix = 'http://thumbs.dreamstime.com/z/image-'\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.dt-pull-center')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.thb-large-gi-box.thb-large-box')\n",
    "\n",
    "    imgnum_pattern = re.compile(r'<strong>(.*?)</strong>')\n",
    "    id_pattern = re.compile(r'id=\"bigthumb(.*?)\" src=') #or re.compile(r'<div id=\"thb_cell(.*?)\"><a')\n",
    "    imgnum_find = re.findall(imgnum_pattern, str(content1))\n",
    "    id_find = re.findall(id_pattern, str(content2))\n",
    "\n",
    "    imgnum = imgnum_find[0].split(',')\n",
    "    imgnum = ''.join(imgnum)\n",
    "    imgnum = int(imgnum) if len(imgnum) else 0\n",
    "    \n",
    "    per_page = 1000\n",
    "    total_pages = (imgnum//per_page + 1) if (imgnum%per_page) else (imgnum//per_page)\n",
    "\n",
    "    for imgid in id_find:\n",
    "        if imgid not in img_id_url_dict:\n",
    "            imgurl = urlprefix + imgid + '.jpg'\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_pond5(content):\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.SearchPage-resultsCount.u-alignTop .js-searchResultsNum')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.SearchResultsV3.js-searchResultsList.js-draggableList .SearchResultV3-thumb')\n",
    "    \n",
    "    imgnum_pattern = re.compile(r'>(.*?)</span>')\n",
    "    url_pattern = re.compile(r'src=\"(.*?)m.jpeg\"')\n",
    "    id_pattern = re.compile(r'net/(.*?)_icon')\n",
    "    \n",
    "    imgnum_find = re.findall(imgnum_pattern, str(content1))\n",
    "    url_find = re.findall(url_pattern, str(content2))\n",
    "    \n",
    "    imgnum = imgnum_find[0].split(',')\n",
    "    imgnum = ''.join(imgnum)\n",
    "    imgnum = int(imgnum) if len(imgnum) else 0\n",
    "    \n",
    "    per_page = 50\n",
    "    total_pages = (imgnum//per_page + 1) if (imgnum%per_page) else (imgnum//per_page)\n",
    "\n",
    "    for eachurl in url_find:\n",
    "        id_find = re.findall(id_pattern, eachurl)\n",
    "        if len(id_find):\n",
    "            imgid = id_find[0]\n",
    "            if imgid not in img_id_url_dict:\n",
    "                imgurl = eachurl + 'l.jpeg'\n",
    "                img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def save_infos(img_id_url_dict, idprefix, fname):\n",
    "    fid = open(fname, 'w')\n",
    "    print type(img_id_url_dict)\n",
    "\n",
    "    for imgid in img_id_url_dict:\n",
    "        imgurl = img_id_url_dict[imgid]\n",
    "        fid.write(idprefix + str(imgid) + '   ' + imgurl + '\\n')\n",
    "\n",
    "    fid.close()\n",
    "\n",
    "def crawler_gettyimages(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(1):\n",
    "        entryurl = entryurl_gettyimages(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_gettyimages(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_flickr(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(1):\n",
    "        entryurl = entryurl_flickr(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_flickr(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_istockphoto(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(1):\n",
    "        entryurl = entryurl_istockphoto(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_istockphoto(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_dreamstime(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(page_index <= 10):\n",
    "        entryurl = entryurl_dreamstime(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            break \n",
    "        total_pages, img_id_url_dict = extract_infos_dreamstime(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_pond5(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(1):\n",
    "        entryurl = entryurl_pond5(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            break  \n",
    "        total_pages, img_id_url_dict = extract_infos_pond5(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "    \n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_wrapper(keyword, image_number, webtype, outdir):\n",
    "    assert webtype in ['gettyimages', 'flickr', 'istockphoto', 'dreamstime', 'pond5'], 'webtype undefined'\n",
    "    outdir = os.path.normpath(outdir + '/' + '_'.join(keyword.split()))\n",
    "    _mkdir_p(outdir)\n",
    "    outfile = outdir + '/' + webtype + '_' + '_'.join(keyword.split()) + '.txt'\n",
    "    idprefix = webtype + '_'\n",
    "\n",
    "    assert os.path.exists(outdir), 'outdir not exist'\n",
    "    assert os.path.isdir(outdir), 'outdir is not a dir'\n",
    "\n",
    "    print 'keyword:', keyword\n",
    "    print 'Need total images:', image_number\n",
    "    print 'output file', outfile\n",
    "    print 'idprefix', idprefix\n",
    "\n",
    "    image_infos = dict()\n",
    "    if webtype == 'gettyimages':\n",
    "        print 'crawl images infos from gettyimages'\n",
    "        image_infos = crawler_gettyimages(keyword, image_number)\n",
    "    elif webtype == 'flickr':\n",
    "        print 'crawl images infos from flickr'\n",
    "        image_infos = crawler_flickr(keyword, image_number)\n",
    "    elif webtype == 'istockphoto':\n",
    "        print 'crawl images infos from istockphoto'\n",
    "        image_infos = crawler_istockphoto(keyword, image_number)\n",
    "    elif webtype == 'dreamstime':\n",
    "        print 'crawl images infos from dreamstime'\n",
    "        image_infos = crawler_dreamstime(keyword, image_number)\n",
    "    elif webtype == 'pond5':\n",
    "        print 'crawl images infos from dreamstime'\n",
    "        image_infos = crawler_pond5(keyword, image_number)\n",
    "\n",
    "    print 'save infos'\n",
    "    save_infos(image_infos, idprefix, outfile)\n",
    "\n",
    "def main(args):\n",
    "    start_time = time.time()\n",
    "    if FLAG_python is True:\n",
    "        # for python run in command-line\n",
    "        webtype = args.webtype\n",
    "        keywords_file = args.keywords_file\n",
    "        image_number = args.image_number\n",
    "        outdir = args.outdir\n",
    "        print 'Python'\n",
    "    else:\n",
    "        # for jupyter\n",
    "        webtype = args['webtype']\n",
    "        keywords_file = args['keywords_file']\n",
    "        image_number = args['image_number']\n",
    "        outdir = args['outdir']\n",
    "        print 'Jupyter'\n",
    "\n",
    "    print '++++++++++++++++++++++++++ START +++++++++++++++++++++++++++++++++'\n",
    "    keywords = _get_infos_from_textfile(keywords_file)\n",
    "    for keyword in keywords:\n",
    "        stime = time.time()\n",
    "        print('------ crawl %s ------' % (keyword))\n",
    "        crawler_wrapper(keyword, image_number, webtype, outdir)\n",
    "        print(\"------ crawl %s cost %s seconds ------\" % (keyword, time.time() - stime))\n",
    "        print ''\n",
    "\n",
    "    print(\"------------- total cost %s seconds ----------\" % (time.time() - start_time))\n",
    "    print '++++++++++++++++++++++++++ DONE ++++++++++++++++++++++++++++++++++'\n",
    "\n",
    "if FLAG_python is True:\n",
    "    if __name__ == \"__main__\":\n",
    "        parser = ArgumentParser(description=\"crawl images\")\n",
    "        parser.add_argument('-webtype', required=True)\n",
    "        parser.add_argument('-keywords_file', required=True)\n",
    "        parser.add_argument('-image_number',type=int,required=True)\n",
    "        parser.add_argument('-outdir', required = True)\n",
    "        args = parser.parse_args()\n",
    "        main(args)\n",
    "else:\n",
    "    args = {}\n",
    "    args['webtype'] = 'pond5'\n",
    "    args['keywords_file'] = '/home/lpzhang/Desktop/crawler/crawl_image_list/mytest.txt'\n",
    "    args['image_number'] = 10\n",
    "    args['outdir'] = \"/home/lpzhang/Desktop/\"\n",
    "    # print args\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
