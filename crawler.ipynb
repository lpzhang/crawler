{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "import errno\n",
    "import random\n",
    "import string\n",
    "from argparse import ArgumentParser\n",
    "import time\n",
    "\n",
    "FLAG_python = False\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "\n",
    "# currently need to change flickr_api_key every day\n",
    "flickr_api_key = '19c834991b41104d12bb1bd40b6e8553'\n",
    "\n",
    "def _mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc: # Python >2.5 (except OSError, exc: for Python <2.5)\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def _get_random_id():\n",
    "    characters = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a','b','c','d','e','f','g','h','i','j','k','l','m','n']\n",
    "    id_element = random.sample(characters, 8)\n",
    "    random_id = ''.join(id_element)\n",
    "    return random_id\n",
    "\n",
    "def _get_infos_from_textfile(fname):\n",
    "    info_list = list()\n",
    "\n",
    "    fid = open(fname, 'r')\n",
    "    lines = fid.readlines()\n",
    "    for eachline in lines:\n",
    "        eachline = eachline.strip()\n",
    "        if not eachline.startswith('#') and len(eachline):\n",
    "            info_list.append(eachline)\n",
    "    fid.close()\n",
    "\n",
    "    return info_list\n",
    "\n",
    "def entryurl_gettyimages(page_index, keyword):\n",
    "    search_keyword_1 = '-'.join(keyword.split())\n",
    "    search_keyword_2 = '%20'.join(keyword.split())\n",
    "    \n",
    "    url1 = 'http://www.gettyimages.com/photos/' + search_keyword_1\n",
    "    url2 = '?excludenudity=true&page=' + str(page_index)\n",
    "    url3 = '&phrase=' + search_keyword_2\n",
    "    url4 = '&sort=best'\n",
    "    url = url1 + url2 + url3 + url4\n",
    "    \n",
    "    return url\n",
    "\n",
    "def entryurl_flickr(page_index, keyword):\n",
    "    search_keyword = '%20'.join(keyword.split())\n",
    "#     search_keyword_2 = '-'.join(keyword.split())\n",
    "\n",
    "    url1 = 'https://api.flickr.com/services/rest?sort=relevance&parse_tags=1&content_type=7&extras=can_comment%2Ccount_comments%2Ccount_faves%2Cdescription%2Cisfavorite%2Clicense%2Cmedia%2Cneeds_interstitial%2Cowner_name%2Cpath_alias%2Crealname%2Crotation%2Curl_c%2Curl_l%2Curl_m%2Curl_n%2Curl_q%2Curl_s%2Curl_sq%2Curl_t%2Curl_z%2Cis_marketplace_licensable&per_page=1000'\n",
    "    url2 = '&page=' + str(page_index)\n",
    "    url3 = '&lang=en-US&dimension_search_mode=min&height=640&width=640&media=photos&text=' + search_keyword\n",
    "    url4 = '&viewerNSID=&method=flickr.photos.search&csrf=&api_key=' + flickr_api_key\n",
    "    url5 = '&format=json&hermes=1&hermesClient=1&reqId=' + _get_random_id()\n",
    "    url6 = '&nojsoncallback=1'\n",
    "    url = url1 + url2 + url3 + url4 + url5 + url6\n",
    "\n",
    "    return url\n",
    "\n",
    "def entryurl_istockphoto(page_index, keyword):\n",
    "    search_keyword_1 = '+'.join(keyword.split())\n",
    "    search_keyword_2 = '%20'.join(keyword.split())\n",
    "\n",
    "    url1 = 'http://www.istockphoto.com/hk/photos/' + search_keyword_1\n",
    "    url2 = '?facets=%7B%22text%22:%5B%22' + search_keyword_2\n",
    "    url3 = '%22%5D,%22pageNumber%22:' + str(page_index)\n",
    "    url4 = ',%22perPage%22:1000,%22abstractType%22:%5B%22photos%22%5D,%22order%22:%22bestMatch%22,%22f%22:true%7D'\n",
    "\n",
    "    url = url1 + url2 + url3 + url4\n",
    "\n",
    "    return url\n",
    "\n",
    "def entryurl_dreamstime(page_index, keyword):\n",
    "    search_keyword = '%20'.join(keyword.split())\n",
    "    \n",
    "    url1 = 'https://www.dreamstime.com/search.php?srh_field=' + search_keyword\n",
    "    url2 = '&s_ph=y&s_st=new&s_sm=all&s_rsf=0&s_rst=7&s_mrg=1&s_sl0=y&s_sl1=y&s_sl2=y&s_sl3=y&s_sl4=y&s_sl5=y&s_clc=y&s_clm=y&s_orp=y&s_ors=y&s_orl=y&s_orw=y&s_mrc1=y&s_mrc2=y&s_mrc3=y&s_mrc4=y&s_mrc5=y&s_exc=&items=1000&pg='\n",
    "    url3 = str(page_index)\n",
    "    url = url1 + url2 + url3\n",
    "\n",
    "    return url\n",
    "\n",
    "def entryurl_pond5(page_index, keyword):\n",
    "    search_keyword = '-'.join(keyword.split())\n",
    "    \n",
    "    url1 = 'https://www.pond5.com/photos/' + str(page_index)\n",
    "    url2 = '/' + search_keyword + '.html'\n",
    "    \n",
    "    url = url1 + url2\n",
    "\n",
    "    return url\n",
    "\n",
    "def entryurl_googleimage(page_index, keyword):\n",
    "    search_keyword = '+'.join(keyword.split())\n",
    "    \n",
    "    url1 = 'https://www.google.com.hk/search?q=' + search_keyword\n",
    "    url2 = '&sa=X&biw=1014&bih=701&tbm=isch&ijn=' + str(page_index)\n",
    "    url3 = '&ei=mA-DV5nNA87o0gSlj5awBQ&start=' + str(page_index*100)\n",
    "    url4 = '&ved=0ahUKEwiZ-celuerNAhVOtJQKHaWHBVYQuT0ILygB&vet=10ahUKEwiZ-celuerNAhVOtJQKHaWHBVYQuT0ILygB.mA-DV5nNA87o0gSlj5awBQ.i'\n",
    "    url = url1 + url2 + url3 + url4\n",
    "    \n",
    "    return url\n",
    "    \n",
    "def entryurl_bingimage(page_index, keyword):\n",
    "    search_keyword = '+'.join(keyword.split())\n",
    "    \n",
    "    url1 = 'http://www.bing.com/images/async?q=' + search_keyword\n",
    "    url2 = '&async=content&first=' + str(page_index)\n",
    "    url3 = '&count=150&dgst=ro_u1871*&IID=images.1&SFX=3&IG=1CD26D97E7534B4DA57E0AFFCD9D7835&CW=1840&CH=447&CT=1467979443818&qft=+filterui:imagesize-medium+filterui:photo-photo&form=R5IR21'\n",
    "    url = url1 + url2 + url3\n",
    "    \n",
    "    return url\n",
    "\n",
    "def response_contents(url):\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36'}\n",
    "    request = urllib2.Request(url, headers = headers)\n",
    "    try:\n",
    "        response = urllib2.urlopen(request)\n",
    "        content = response.read()\n",
    "    except urllib2.HTTPError as err:\n",
    "        content = ''\n",
    "        if err.code == 404:\n",
    "            print 'HTTP Error 404: Not Found'\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    return content\n",
    "\n",
    "def extract_infos_gettyimages(content):\n",
    "    urlprefix = 'http://media.gettyimages.com/photos/id'\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.full.pagination')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.details-wrap .asset-link')\n",
    "\n",
    "    pages_pattern = re.compile(r'page-count=\"(\\d*)\"')\n",
    "    id_pattern = re.compile(r'data-asset-id=\"(.*?)\"')\n",
    "    pages_find = re.findall(pages_pattern, str(content1))\n",
    "    id_find = re.findall(id_pattern, str(content2))\n",
    "\n",
    "    total_pages = int(pages_find[0]) if len(pages_find) else 0\n",
    "\n",
    "    for imgid in id_find:\n",
    "        if imgid not in img_id_url_dict:\n",
    "            imgurl = urlprefix + imgid\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_flickr(content):\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content = BeautifulSoup(content,\"lxml\").find('p').getText()\n",
    "\n",
    "    pages_pattern = re.compile(r'\"pages\":(\\d*),')\n",
    "    url_pattern = re.compile(r',\"url_l\":\"(.*?)\\.jpg\",')\n",
    "    id_pattern = re.compile(r'\\/(\\d*)_')\n",
    "    pages_find = re.findall(pages_pattern, str(content))\n",
    "    url_find = re.findall(url_pattern, str(content))\n",
    "\n",
    "\n",
    "    total_pages = int(pages_find[0]) if len(pages_find) else 0\n",
    "\n",
    "    for eachurl in url_find:\n",
    "        id_find = re.findall(id_pattern, eachurl)\n",
    "        if len(id_find):\n",
    "            imgid = id_find[0]\n",
    "            if imgid not in img_id_url_dict:\n",
    "                imgurl = eachurl.replace(\"\\/\", \"/\") + '.jpg'\n",
    "                img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_istockphoto(content):\n",
    "    urlprefix = 'http://media.istockphoto.com/photos/id'\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.file-count-label')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.figure-holder')\n",
    "\n",
    "    imgnum_pattern = re.compile(r'>(\\d*)</span>')\n",
    "    id_pattern = re.compile(r'-gm(\\d*)-')\n",
    "    imgnum_find = re.findall(imgnum_pattern, str(content1))\n",
    "    id_find = re.findall(id_pattern, str(content2))\n",
    "\n",
    "    imgnum = imgnum_find[0].split(',')\n",
    "    imgnum = ''.join(imgnum)\n",
    "    imgnum = int(imgnum) if len(imgnum) else 0\n",
    "\n",
    "    per_page = 1000\n",
    "    total_pages = (imgnum//per_page + 1) if (imgnum%per_page) else (imgnum//per_page)\n",
    "\n",
    "    for imgid in id_find:\n",
    "        if imgid not in img_id_url_dict:\n",
    "            imgurl = urlprefix + imgid\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_dreamstime(content):\n",
    "    urlprefix = 'http://thumbs.dreamstime.com/z/image-'\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.dt-pull-center')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.thb-large-gi-box.thb-large-box')\n",
    "\n",
    "    imgnum_pattern = re.compile(r'<strong>(.*?)</strong>')\n",
    "    id_pattern = re.compile(r'id=\"bigthumb(.*?)\" src=') #or re.compile(r'<div id=\"thb_cell(.*?)\"><a')\n",
    "    imgnum_find = re.findall(imgnum_pattern, str(content1))\n",
    "    id_find = re.findall(id_pattern, str(content2))\n",
    "\n",
    "    imgnum = imgnum_find[0].split(',')\n",
    "    imgnum = ''.join(imgnum)\n",
    "    imgnum = int(imgnum) if len(imgnum) else 0\n",
    "\n",
    "    per_page = 1000\n",
    "    total_pages = (imgnum//per_page + 1) if (imgnum%per_page) else (imgnum//per_page)\n",
    "\n",
    "    for imgid in id_find:\n",
    "        if imgid not in img_id_url_dict:\n",
    "            imgurl = urlprefix + imgid + '.jpg'\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_pond5(content):\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.SearchPage-resultsCount.u-alignTop .js-searchResultsNum')\n",
    "    content2 = BeautifulSoup(content,\"lxml\").select('.SearchResultsV3.js-searchResultsList.js-draggableList .SearchResultV3-thumb')\n",
    "\n",
    "    imgnum_pattern = re.compile(r'>(.*?)</span>')\n",
    "    url_pattern = re.compile(r'src=\"(.*?)m.jpeg\"')\n",
    "    id_pattern = re.compile(r'net/(.*?)_icon')\n",
    "\n",
    "    imgnum_find = re.findall(imgnum_pattern, str(content1))\n",
    "    url_find = re.findall(url_pattern, str(content2))\n",
    "\n",
    "    imgnum = imgnum_find[0].split(',')\n",
    "    imgnum = ''.join(imgnum)\n",
    "    imgnum = int(imgnum) if len(imgnum) else 0\n",
    "\n",
    "    per_page = 50\n",
    "    total_pages = (imgnum//per_page + 1) if (imgnum%per_page) else (imgnum//per_page)\n",
    "\n",
    "    for eachurl in url_find:\n",
    "        id_find = re.findall(id_pattern, eachurl)\n",
    "        if len(id_find):\n",
    "            imgid = id_find[0]\n",
    "            if imgid not in img_id_url_dict:\n",
    "                imgurl = eachurl + 'l.jpeg'\n",
    "                img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return total_pages, img_id_url_dict\n",
    "\n",
    "def extract_infos_googleimage(content):\n",
    "    img_id_url_dict = dict()\n",
    "    \n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.rg_di.rg_el.ivg-i .rg_meta')\n",
    "    ou_pattern = re.compile(r'\"ou\":\"(.*?)\",\"ow\"')\n",
    "    id_pattern = re.compile(r'\"id\":\"(.*?):\",\"isu\"')\n",
    "    ity_pattern = re.compile(r'\"ity\":\"(.*?)\",\"oh\"')\n",
    "    url_pattern = re.compile(r'(.*?).jpg')\n",
    "    \n",
    "    for item in content1:\n",
    "        item = str(item)\n",
    "        ou_find = re.findall(ou_pattern, item)\n",
    "        id_find = re.findall(id_pattern, item)\n",
    "        \n",
    "        if len(id_find)==0 or len(ou_find)==0:\n",
    "            continue\n",
    "        ou_find = ou_find[0]\n",
    "        imgid = id_find[0]\n",
    "        ity_find = re.findall(ity_pattern, item)\n",
    "        if len(ity_find):\n",
    "            url_find = re.findall(url_pattern, ou_find)\n",
    "            if len(url_find)==0:\n",
    "                continue\n",
    "            imgurl = url_find[0] + '.jpg'\n",
    "        if imgid not in img_id_url_dict:\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "\n",
    "    return 0, img_id_url_dict\n",
    "\n",
    "def extract_infos_bingimage(content):\n",
    "    img_id_url_dict = dict()\n",
    "\n",
    "    content1 = BeautifulSoup(content,\"lxml\").select('.imgres .dg_u')\n",
    "    id_pattern = re.compile(r'md5:\"(.*?)\",surl:')\n",
    "    url_pattern = re.compile(r'imgurl:\"(.*?)\",tid:')\n",
    "    for item in content1:\n",
    "        item = str(item)\n",
    "        id_find = re.findall(id_pattern, item)\n",
    "        url_find = re.findall(url_pattern, item)\n",
    "        if len(id_find)==0 or len(url_find)==0:\n",
    "            continue\n",
    "        imgid = id_find[0]\n",
    "        imgurl = url_find[0]\n",
    "        if imgid not in img_id_url_dict:\n",
    "            img_id_url_dict[imgid] = imgurl\n",
    "            \n",
    "    return 0, img_id_url_dict\n",
    "\n",
    "def save_infos(img_id_url_dict, idprefix, fname):\n",
    "    fid = open(fname, 'w')\n",
    "\n",
    "    for imgid in img_id_url_dict:\n",
    "        imgurl = img_id_url_dict[imgid]\n",
    "        fid.write(idprefix + str(imgid) + '   ' + imgurl + '\\n')\n",
    "\n",
    "    fid.close()\n",
    "\n",
    "def crawler_gettyimages(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(1):\n",
    "        entryurl = entryurl_gettyimages(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            print 'contents are empty'\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_gettyimages(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_flickr(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(page_index < 50):\n",
    "        entryurl = entryurl_flickr(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            print 'contents are empty'\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_flickr(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_istockphoto(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(1):\n",
    "        entryurl = entryurl_istockphoto(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            print 'contents are empty'\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_istockphoto(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_dreamstime(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(page_index <= 10):\n",
    "        entryurl = entryurl_dreamstime(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            print 'contents are empty'\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_dreamstime(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_pond5(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(1):\n",
    "        entryurl = entryurl_pond5(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            print 'contents are empty'\n",
    "            break\n",
    "        total_pages, img_id_url_dict = extract_infos_pond5(contents)\n",
    "\n",
    "        if (total_pages == 0):\n",
    "            break\n",
    "        print('****** current page %d (%d) ******' % (page_index, total_pages))\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        if (page_index > total_pages):\n",
    "            print('current page %d is last pages' % (page_index))\n",
    "            break\n",
    "\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_googleimage(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(page_index <= 10):\n",
    "        entryurl = entryurl_googleimage(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            print 'contents are empty'\n",
    "            break\n",
    "            \n",
    "        total_pages, img_id_url_dict = extract_infos_googleimage(contents)\n",
    "\n",
    "        print('****** current page %d ******' % page_index)\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_bingimage(keyword, image_number):\n",
    "    image_id_url_dict = dict()\n",
    "    page_index = 1\n",
    "\n",
    "    while(page_index <= 10):\n",
    "        entryurl = entryurl_bingimage(page_index, keyword)\n",
    "        contents = response_contents(entryurl)\n",
    "        if len(contents) == 0:\n",
    "            print 'contents are empty'\n",
    "            break\n",
    "            \n",
    "        total_pages, img_id_url_dict = extract_infos_bingimage(contents)\n",
    "        \n",
    "        print('****** current page %d ******' % page_index)\n",
    "\n",
    "        image_id_url_dict.update(img_id_url_dict)\n",
    "        if (image_number < len(image_id_url_dict)):\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "        print('%d more images need to crawl' % (image_number - len(image_id_url_dict)))\n",
    "\n",
    "    return image_id_url_dict\n",
    "\n",
    "def crawler_wrapper(keyword, image_number, webtype, outfile):\n",
    "    idprefix = webtype + '_'\n",
    "    \n",
    "    image_infos = dict()\n",
    "    if webtype == 'gettyimages':\n",
    "        print 'crawl images infos from gettyimages'\n",
    "        image_infos = crawler_gettyimages(keyword, image_number)\n",
    "    elif webtype == 'flickr':\n",
    "        print 'crawl images infos from flickr'\n",
    "        image_infos = crawler_flickr(keyword, image_number)\n",
    "    elif webtype == 'istockphoto':\n",
    "        print 'crawl images infos from istockphoto'\n",
    "        image_infos = crawler_istockphoto(keyword, image_number)\n",
    "    elif webtype == 'dreamstime':\n",
    "        print 'crawl images infos from dreamstime'\n",
    "        image_infos = crawler_dreamstime(keyword, image_number)\n",
    "    elif webtype == 'pond5':\n",
    "        print 'crawl images infos from dreamstime'\n",
    "        image_infos = crawler_pond5(keyword, image_number)\n",
    "    elif webtype == 'googleimage':\n",
    "        print 'crawl images infos from dreamstime'\n",
    "        image_infos = crawler_googleimage(keyword, image_number)\n",
    "    elif webtype == 'bingimage':\n",
    "        print 'crawl images infos from dreamstime'\n",
    "        image_infos = crawler_bingimage(keyword, image_number)\n",
    "\n",
    "    # truncate image_infos\n",
    "    while (len(image_infos) > image_number):\n",
    "        image_infos.popitem()\n",
    "    print 'truncated image_infos size:', len(image_infos)\n",
    "        \n",
    "    print 'save infos'\n",
    "    save_infos(image_infos, idprefix, outfile)\n",
    "\n",
    "def main(args):\n",
    "    start_time = time.time()\n",
    "    if FLAG_python is True:\n",
    "        # for python run in command-line\n",
    "        webtype = args.webtype\n",
    "        keywords_file = args.keywords_file\n",
    "        image_number = args.image_number\n",
    "        outdir = args.outdir\n",
    "        print 'Python'\n",
    "    else:\n",
    "        # for jupyter\n",
    "        webtype = args['webtype']\n",
    "        keywords_file = args['keywords_file']\n",
    "        image_number = args['image_number']\n",
    "        outdir = args['outdir']\n",
    "        print 'Jupyter'\n",
    "        \n",
    "    # check the args\n",
    "    if not os.path.isabs(keywords_file):\n",
    "        keywords_file = os.path.abspath(keywords_file)\n",
    "    if not os.path.isabs(outdir):\n",
    "        outdir = os.path.abspath(outdir)\n",
    "    print 'webtype:', webtype\n",
    "    print 'image_number:', image_number\n",
    "    print 'keywords_file:', keywords_file\n",
    "    print 'outdir:', outdir\n",
    "    assert webtype in ['gettyimages', 'flickr', 'istockphoto', 'dreamstime', 'pond5', 'googleimage', 'bingimage'], 'webtype undefined'\n",
    "    assert type(image_number) is int, 'image_number must be integer'\n",
    "    assert (image_number > 0 and image_number < 20000), 'image_number must more than 0 and less than 20000'\n",
    "    assert os.path.exists(keywords_file), 'keywords_file not exist'\n",
    "    assert os.path.isfile(keywords_file), 'keywords_file is not a file'\n",
    "    assert os.path.exists(outdir), 'outdir not exist'\n",
    "    assert os.path.isdir(outdir), 'outdir is not a dir'\n",
    "    print 'args checked\\n'\n",
    "    \n",
    "    print '++++++++++++++++++++++++++ START +++++++++++++++++++++++++++++++++'\n",
    "    outfilelist = list()\n",
    "    index = 0\n",
    "    keywords = _get_infos_from_textfile(keywords_file)\n",
    "    for keyword in keywords:\n",
    "        # create directory for store results\n",
    "        keyworddir = os.path.normpath(outdir + '/' + '_'.join(keyword.split()))\n",
    "        _mkdir_p(keyworddir)\n",
    "        assert os.path.exists(keyworddir), 'keyworddir not exist'\n",
    "        assert os.path.isdir(keyworddir), 'keyworddir is not a dir'\n",
    "        # outfile for store images id and url\n",
    "        outfile = keyworddir + '/' + webtype + '_' + '_'.join(keyword.split()) + '.txt'\n",
    "        ###### begin for crawling each keyword images in website ######\n",
    "        stime = time.time()\n",
    "        index += 1\n",
    "        print('------ begin crawl %d(%d) ------' % (index, len(keywords)))\n",
    "        print 'keyword:', keyword\n",
    "        print 'Need total images:', image_number\n",
    "        print 'crawl from website:', webtype\n",
    "        print 'output file:', outfile\n",
    "        crawler_wrapper(keyword, image_number, webtype, outfile)\n",
    "        outfilelist.append(outfile)\n",
    "        print(\"------ crawl %s cost %s seconds ------\" % (keyword, time.time() - stime))\n",
    "        print ''\n",
    "        ###### end for crawling each keyword images in website ######\n",
    "        \n",
    "    # save outfilelist for farther download\n",
    "    outfpath = os.path.splitext(keywords_file)[0] + '.download'\n",
    "    print 'save outfilelist:', outfpath\n",
    "    outffid = open(outfpath,'w')\n",
    "    for outf in outfilelist:\n",
    "        outffid.write(outf + '\\n')\n",
    "    outffid.close()\n",
    "    \n",
    "    print(\"------------- total cost %s seconds ----------\" % (time.time() - start_time))\n",
    "    print '++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++'\n",
    "\n",
    "if FLAG_python is True:\n",
    "    if __name__ == \"__main__\":\n",
    "        parser = ArgumentParser(description=\"crawl images\")\n",
    "        parser.add_argument('-webtype', required=True)\n",
    "        parser.add_argument('-keywords_file', required=True)\n",
    "        parser.add_argument('-image_number',type=int,required=True)\n",
    "        parser.add_argument('-outdir', required = True)\n",
    "        args = parser.parse_args()\n",
    "        main(args)\n",
    "else:\n",
    "    args = {}\n",
    "    args['webtype'] = 'googleimage'\n",
    "    args['keywords_file'] = 'mytest.txt'\n",
    "    args['image_number'] = 10\n",
    "    args['outdir'] = \"test\"\n",
    "    # print args\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
